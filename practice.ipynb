{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Practice on Keras Functional API"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db106a0ce89886c6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:10:34.233566900Z",
     "start_time": "2023-08-14T19:10:30.243755400Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# mnist dataset\n",
    "# loading training MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# reshaping and normalizing the images\n",
    "x_train = x_train.reshape(60000, 784).astype('float32')/255  \n",
    "x_test = x_test.reshape(10000, 784).astype('float32')/255\n",
    "# one-hot encoding of labels\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:10:34.575065700Z",
     "start_time": "2023-08-14T19:10:34.237067100Z"
    }
   },
   "id": "a51d67e12619bd7d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 784)]             0         \n",
      "                                                                 \n",
      " hidden1 (Dense)             (None, 64)                50240     \n",
      "                                                                 \n",
      " hidden2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55050 (215.04 KB)\n",
      "Trainable params: 55050 (215.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building the model\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Build a simple MLP model for MNIST classification.\n",
    "    :return: A Keras Model with inputs and outputs defined.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(784,), name='input')  # input layer\n",
    "    x = layers.Dense(64, activation='relu', name='hidden1')(inputs)  # hidden layer\n",
    "    x = layers.Dense(64, activation='relu', name='hidden2')(x)  # hidden layer\n",
    "    outputs = layers.Dense(10, activation='softmax', name='output')(x)  # output layer\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_ce = build_model()\n",
    "model_fl = build_model()\n",
    "model_rl = build_model()\n",
    "\n",
    "model_rl.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:10:34.737065100Z",
     "start_time": "2023-08-14T19:10:34.578065400Z"
    }
   },
   "id": "5a49bafd834711d3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 1.2152 - categorical_accuracy: 0.6551 - val_loss: 0.5322 - val_categorical_accuracy: 0.8586\n",
      "Epoch 2/6\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.4537 - categorical_accuracy: 0.8767 - val_loss: 0.3591 - val_categorical_accuracy: 0.8995\n",
      "Epoch 3/6\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.3532 - categorical_accuracy: 0.9017 - val_loss: 0.3110 - val_categorical_accuracy: 0.9106\n",
      "Epoch 4/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.3117 - categorical_accuracy: 0.9120 - val_loss: 0.2805 - val_categorical_accuracy: 0.9205\n",
      "Epoch 5/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.2856 - categorical_accuracy: 0.9182 - val_loss: 0.2602 - val_categorical_accuracy: 0.9252\n",
      "Epoch 6/6\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.2662 - categorical_accuracy: 0.9238 - val_loss: 0.2482 - val_categorical_accuracy: 0.9310\n",
      "313/313 - 0s - loss: 0.2506 - categorical_accuracy: 0.9280 - 319ms/epoch - 1ms/step\n",
      "Test loss: 0.2505910396575928\n",
      "Test accuracy: 0.9279999732971191\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD()\n",
    "ce_loss = keras.losses.CategoricalCrossentropy()\n",
    "metrics = [keras.metrics.CategoricalAccuracy()]\n",
    "# compiling the model\n",
    "model_ce.compile(optimizer=optimizer, loss=ce_loss, metrics=metrics)\n",
    "# training the model\n",
    "model_ce.fit(x_train, y_train, batch_size=64, epochs=6, validation_split=0.2)\n",
    "# evaluating the model\n",
    "test_scores = model_ce.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])\n",
    "\n",
    "# saving the model\n",
    "path = './weights/mnist_ce_model.keras'\n",
    "model_ce.save(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:10:45.274065900Z",
     "start_time": "2023-08-14T19:10:34.733566100Z"
    }
   },
   "id": "74658d8b5c82aac1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.3514 - categorical_accuracy: 0.5706 - val_loss: 0.2039 - val_categorical_accuracy: 0.7633\n",
      "Epoch 2/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.1413 - categorical_accuracy: 0.7988 - val_loss: 0.0950 - val_categorical_accuracy: 0.8461\n",
      "Epoch 3/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0870 - categorical_accuracy: 0.8428 - val_loss: 0.0707 - val_categorical_accuracy: 0.8645\n",
      "Epoch 4/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0701 - categorical_accuracy: 0.8625 - val_loss: 0.0606 - val_categorical_accuracy: 0.8773\n",
      "Epoch 5/6\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0617 - categorical_accuracy: 0.8731 - val_loss: 0.0550 - val_categorical_accuracy: 0.8862\n",
      "Epoch 6/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 0.0565 - categorical_accuracy: 0.8814 - val_loss: 0.0511 - val_categorical_accuracy: 0.8911\n",
      "313/313 - 0s - loss: 0.0504 - categorical_accuracy: 0.8923 - 311ms/epoch - 992us/step\n",
      "Test loss: 0.050351034849882126\n",
      "Test accuracy: 0.892300009727478\n"
     ]
    }
   ],
   "source": [
    "focal_loss = keras.losses.CategoricalFocalCrossentropy()\n",
    "# compiling the model\n",
    "optimizer = keras.optimizers.SGD()\n",
    "model_fl.compile(optimizer=optimizer, loss=focal_loss, metrics=metrics)\n",
    "# training the model\n",
    "model_fl.fit(x_train, y_train, batch_size=64, epochs=6, validation_split=0.2)\n",
    "# evaluating the model\n",
    "test_scores = model_fl.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])\n",
    "# saving the model\n",
    "path = './weights/mnist_focal_model.keras'\n",
    "model_fl.save(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:10:56.127567900Z",
     "start_time": "2023-08-14T19:10:45.277067100Z"
    }
   },
   "id": "ff849813ffddfd76"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# # defining focal  loss function\n",
    "# # FL(p_t) = -alpha_t * (1-p_t)^gamma * log(p_t)\n",
    "# def focal_loss(alpha=0.25, gamma=2.0):\n",
    "#     \"\"\"\n",
    "#     Focal Loss, Keras styles\n",
    "#     :param alpha: Weighting factor for the positive class, typically in the range [0, 1].\n",
    "#     :param gamma: Focusing parameter to down-weight well-classified examples, typically in the range [0, 5].\n",
    "#     :return: A callable focal_loss_fixed(y_true, y_pred) to be used as a Keras loss function.\n",
    "#     \"\"\"\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         \"\"\"\n",
    "#         The actual loss computation.\n",
    "#         :param y_true: Ground truth labels, shape of [batch_size, num_classes].\n",
    "#         :param y_pred: Predicted class probabilities, shape of [batch_size, num_classes].\n",
    "#         :return: \n",
    "#         \"\"\"\n",
    "#         # Clip the prediction value to prevent NaN's and Inf's\n",
    "#         y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())\n",
    "#         # Calculate Cross Entropy\n",
    "#         cross_entropy = -y_true * K.log(y_pred)\n",
    "#         # Calculate Focal Loss\n",
    "#         loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "#         # Compute mean loss in batch\n",
    "#         return K.mean(K.sum(loss, axis=-1))\n",
    "#     return focal_loss_fixed\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:10:56.146066900Z",
     "start_time": "2023-08-14T19:10:56.122069Z"
    }
   },
   "id": "9a9fca6fe2cea7ec"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# defining rational loss function\n",
    "# RL(p_t) = - 1/p_t * log(p_t)\n",
    "def rational_loss():\n",
    "    \"\"\"\n",
    "    Rational Loss for multi-class classification, Keras style.\n",
    "    RL(p_t) = - 1/p_t * log(p_t), where p_t is the probability associated with the true class.\n",
    "\n",
    "    :return: A callable rational_loss_fixed(y_true, y_pred) to be used as a Keras loss function.\n",
    "    \"\"\"\n",
    "    def rational_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        The actual loss computation.\n",
    "        :param y_true: Ground truth labels, shape of [batch_size, num_classes].\n",
    "        :param y_pred: Predicted class probabilities, shape of [batch_size, num_classes].\n",
    "        :return: A scalar representing the mean rational loss over the batch.\n",
    "        \"\"\"\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        # Compute the Rational Loss\n",
    "        loss = (1 / y_pred) * cross_entropy\n",
    "        # Average the loss over the batch\n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "    # TODO: use builtin categorical crossentropy\n",
    "    # TODO: plot the function\n",
    "    \n",
    "\n",
    "    return rational_loss_fixed\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T20:14:50.809484100Z",
     "start_time": "2023-08-14T20:14:50.735488100Z"
    }
   },
   "id": "eb292d1ae3a65bc1"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 136753584.0000 - categorical_accuracy: 0.2270 - val_loss: 144498704.0000 - val_categorical_accuracy: 0.1035\n",
      "Epoch 2/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 144764128.0000 - categorical_accuracy: 0.1019 - val_loss: 144498704.0000 - val_categorical_accuracy: 0.1035\n",
      "Epoch 3/6\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 144764144.0000 - categorical_accuracy: 0.1019 - val_loss: 144498704.0000 - val_categorical_accuracy: 0.1035\n",
      "Epoch 4/6\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 144764160.0000 - categorical_accuracy: 0.1019 - val_loss: 144498704.0000 - val_categorical_accuracy: 0.1035\n",
      "Epoch 5/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 144764064.0000 - categorical_accuracy: 0.1019 - val_loss: 144498704.0000 - val_categorical_accuracy: 0.1035\n",
      "Epoch 6/6\n",
      "750/750 [==============================] - 2s 2ms/step - loss: 144764080.0000 - categorical_accuracy: 0.1019 - val_loss: 144498704.0000 - val_categorical_accuracy: 0.1035\n",
      "313/313 - 0s - loss: 144901712.0000 - categorical_accuracy: 0.1010 - 377ms/epoch - 1ms/step\n",
      "Test loss: 144901712.0\n",
      "Test accuracy: 0.10100000351667404\n"
     ]
    }
   ],
   "source": [
    "# compiling the model\n",
    "rational_loss = rational_loss()\n",
    "optimizer = keras.optimizers.SGD()\n",
    "model_rl.compile(optimizer=optimizer, loss=rational_loss, metrics=metrics)\n",
    "# training the model\n",
    "history = model_rl.fit(x_train, y_train, batch_size=64, epochs=6, validation_split=0.2)\n",
    "# evaluating the model\n",
    "test_scores = model_rl.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])\n",
    "# saving the model\n",
    "path = './weights/mnist_rational_model.keras'\n",
    "model_rl.save(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T20:15:05.834496200Z",
     "start_time": "2023-08-14T20:14:52.952498400Z"
    }
   },
   "id": "25ab1b90eb796366"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# rebuilding the models\n",
    "model_ce = build_model()\n",
    "model_fl = build_model()\n",
    "model_rl = build_model()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:11:10.501568300Z",
     "start_time": "2023-08-14T19:11:10.501066700Z"
    }
   },
   "id": "3b0b6284c01bdf98"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# imbalance \n",
    "def create_imbalanced_data(x, y, imbalance_rate=0.5):\n",
    "    \"\"\"\n",
    "    Create an imbalanced dataset based on a given probability distribution.\n",
    "    The probability for class d is given by: P(d) = 0.5^d / 2*(1 - 0.5^10)\n",
    "\n",
    "    :param x: Features, shape of [total_samples, feature_dim].\n",
    "    :param y: One-hot encoded labels, shape of [total_samples, num_classes].\n",
    "    :param imbalance_rate: Base rate for the exponential decay of class frequency (default 0.5).\n",
    "    :return: Tuple of imbalanced features and labels, shapes of [selected_samples, feature_dim] and [selected_samples, num_classes].\n",
    "    \"\"\"\n",
    "    total_samples = len(y)\n",
    "    a = imbalance_rate\n",
    "    normalization_factor = 2 * (1 - a**10)\n",
    "\n",
    "    indices_by_class = [np.where(y[:, d] == 1)[0] for d in range(10)]\n",
    "    selected_indices = []\n",
    "\n",
    "    for d in range(10):\n",
    "        probability_d = (a**d) / normalization_factor\n",
    "        frequency = int(total_samples * probability_d)\n",
    "        np.random.shuffle(indices_by_class[d]) # Shuffle to ensure random selection\n",
    "        selected_indices.extend(indices_by_class[d][:frequency])\n",
    "\n",
    "    return x[selected_indices], y[selected_indices]\n",
    "\n",
    "\n",
    "x_train_imbalanced, y_train_imbalanced = create_imbalanced_data(x_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:11:10.503565Z",
     "start_time": "2023-08-14T19:11:10.501066700Z"
    }
   },
   "id": "674c73a18c8f5973"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model_ce.compile(optimizer=keras.optimizers.SGD(), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
    "model_fl.compile(optimizer=keras.optimizers.SGD(), loss=keras.losses.CategoricalFocalCrossentropy(), metrics=metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:16:37.627528500Z",
     "start_time": "2023-08-14T19:16:37.576533600Z"
    }
   },
   "id": "16d76b12905e4aa4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rational_loss_fixed() missing 2 required positional arguments: 'y_true' and 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m rational_loss \u001B[38;5;241m=\u001B[39m \u001B[43mrational_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mSGD()\n\u001B[0;32m      3\u001B[0m model_rl\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer, loss\u001B[38;5;241m=\u001B[39mrational_loss, metrics\u001B[38;5;241m=\u001B[39mmetrics)\n",
      "\u001B[1;31mTypeError\u001B[0m: rational_loss_fixed() missing 2 required positional arguments: 'y_true' and 'y_pred'"
     ]
    }
   ],
   "source": [
    "rational_loss = rational_loss()\n",
    "optimizer = keras.optimizers.SGD()\n",
    "model_rl.compile(optimizer=optimizer, loss=rational_loss, metrics=metrics)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:13:10.116320700Z",
     "start_time": "2023-08-14T19:13:10.033827Z"
    }
   },
   "id": "505f049936394d07"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on imbalanced data:\n",
      "Cross Entropy:\n",
      "Epoch 1/10\n",
      "815/815 [==============================] - 2s 2ms/step - loss: 0.6434 - categorical_accuracy: 0.6245\n",
      "Epoch 2/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.2728 - categorical_accuracy: 0.9224\n",
      "Epoch 3/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.2185 - categorical_accuracy: 0.9380\n",
      "Epoch 4/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.1906 - categorical_accuracy: 0.9449\n",
      "Epoch 5/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.1719 - categorical_accuracy: 0.9497\n",
      "Epoch 6/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.1582 - categorical_accuracy: 0.9535\n",
      "Epoch 7/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.1471 - categorical_accuracy: 0.9570\n",
      "Epoch 8/10\n",
      "815/815 [==============================] - 2s 2ms/step - loss: 0.1378 - categorical_accuracy: 0.9597\n",
      "Epoch 9/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.1298 - categorical_accuracy: 0.9624\n",
      "Epoch 10/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.1224 - categorical_accuracy: 0.9644\n",
      "Focal Loss:\n",
      "Epoch 1/10\n",
      "815/815 [==============================] - 2s 2ms/step - loss: 0.2008 - categorical_accuracy: 0.8294\n",
      "Epoch 2/10\n",
      "815/815 [==============================] - 2s 2ms/step - loss: 0.0800 - categorical_accuracy: 0.8718\n",
      "Epoch 3/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.0571 - categorical_accuracy: 0.8964\n",
      "Epoch 4/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.0472 - categorical_accuracy: 0.9127\n",
      "Epoch 5/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.0416 - categorical_accuracy: 0.9222\n",
      "Epoch 6/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.0379 - categorical_accuracy: 0.9276\n",
      "Epoch 7/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.0352 - categorical_accuracy: 0.9323\n",
      "Epoch 8/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.0331 - categorical_accuracy: 0.9356\n",
      "Epoch 9/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.0314 - categorical_accuracy: 0.9377\n",
      "Epoch 10/10\n",
      "815/815 [==============================] - 1s 2ms/step - loss: 0.0301 - categorical_accuracy: 0.9395\n",
      "Rational Loss:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m model_fl\u001B[38;5;241m.\u001B[39mfit(x_train_imbalanced, y_train_imbalanced, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRational Loss:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mmodel_rl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train_imbalanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_imbalanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\keras-api\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\keras-api\\lib\\site-packages\\keras\\src\\engine\\training.py:3875\u001B[0m, in \u001B[0;36mModel._assert_compile_was_called\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   3869\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assert_compile_was_called\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   3870\u001B[0m     \u001B[38;5;66;03m# Checks whether `compile` has been called. If it has been called,\u001B[39;00m\n\u001B[0;32m   3871\u001B[0m     \u001B[38;5;66;03m# then the optimizer is set. This is different from whether the\u001B[39;00m\n\u001B[0;32m   3872\u001B[0m     \u001B[38;5;66;03m# model is compiled\u001B[39;00m\n\u001B[0;32m   3873\u001B[0m     \u001B[38;5;66;03m# (i.e. whether the model is built and its inputs/outputs are set).\u001B[39;00m\n\u001B[0;32m   3874\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_compiled:\n\u001B[1;32m-> 3875\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   3876\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou must compile your model before \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3877\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining/testing. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3878\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse `model.compile(optimizer, loss)`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3879\u001B[0m         )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "print(\"Training on imbalanced data:\")\n",
    "print(\"Cross Entropy:\")\n",
    "model_ce.fit(x_train_imbalanced, y_train_imbalanced, epochs=10, batch_size=32)\n",
    "print(\"Focal Loss:\")\n",
    "model_fl.fit(x_train_imbalanced, y_train_imbalanced, epochs=10, batch_size=32)\n",
    "print(\"Rational Loss:\")\n",
    "model_rl.fit(x_train_imbalanced, y_train_imbalanced, epochs=10, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:17:11.784570500Z",
     "start_time": "2023-08-14T19:16:40.843576600Z"
    }
   },
   "id": "bdefdf376d35ec93"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6645 - categorical_accuracy: 0.7812\n",
      "[0.6644544005393982, 0.7811999917030334]\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2202 - categorical_accuracy: 0.7008\n",
      "[0.2201809138059616, 0.7008000016212463]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(model_ce\u001B[38;5;241m.\u001B[39mevaluate(x_test, y_test))\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(model_fl\u001B[38;5;241m.\u001B[39mevaluate(x_test, y_test))\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel_rl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Imbalanced data\u001B[39;00m\n\u001B[0;32m      7\u001B[0m x_test_imbalanced, y_test_imbalanced \u001B[38;5;241m=\u001B[39m create_imbalanced_data(x_test, y_test)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\keras-api\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\keras-api\\lib\\site-packages\\keras\\src\\engine\\training.py:3875\u001B[0m, in \u001B[0;36mModel._assert_compile_was_called\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   3869\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assert_compile_was_called\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   3870\u001B[0m     \u001B[38;5;66;03m# Checks whether `compile` has been called. If it has been called,\u001B[39;00m\n\u001B[0;32m   3871\u001B[0m     \u001B[38;5;66;03m# then the optimizer is set. This is different from whether the\u001B[39;00m\n\u001B[0;32m   3872\u001B[0m     \u001B[38;5;66;03m# model is compiled\u001B[39;00m\n\u001B[0;32m   3873\u001B[0m     \u001B[38;5;66;03m# (i.e. whether the model is built and its inputs/outputs are set).\u001B[39;00m\n\u001B[0;32m   3874\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_compiled:\n\u001B[1;32m-> 3875\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m   3876\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou must compile your model before \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3877\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining/testing. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3878\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse `model.compile(optimizer, loss)`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3879\u001B[0m         )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "# Balanced data\n",
    "print(model_ce.evaluate(x_test, y_test))\n",
    "print(model_fl.evaluate(x_test, y_test))\n",
    "print(model_rl.evaluate(x_test, y_test))\n",
    "\n",
    "# Imbalanced data\n",
    "x_test_imbalanced, y_test_imbalanced = create_imbalanced_data(x_test, y_test)\n",
    "print(model_ce.evaluate(x_test_imbalanced, y_test_imbalanced))\n",
    "print(model_fl.evaluate(x_test_imbalanced, y_test_imbalanced))\n",
    "print(model_rl.evaluate(x_test_imbalanced, y_test_imbalanced))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:19:39.015391700Z",
     "start_time": "2023-08-14T19:19:37.656399200Z"
    }
   },
   "id": "c7d9401ce9ef65d9"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by bins for balanced data:\n",
      "Cross Entropy:\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Accuracy for bin 0-1: 0.9929078014184397\n",
      "Accuracy for bin 2-7: 0.9010504913588614\n",
      "Accuracy for bin 8-9: 0.19868885526979324\n",
      "Focal Loss:\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Accuracy for bin 0-1: 0.991016548463357\n",
      "Accuracy for bin 2-7: 0.8309047780413419\n",
      "Accuracy for bin 8-9: 0.004034291477559254\n",
      "Rational Loss:\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Accuracy for bin 0-1: 0.17494089834515367\n",
      "Accuracy for bin 2-7: 0.17705862419518809\n",
      "Accuracy for bin 8-9: 0.0\n"
     ]
    }
   ],
   "source": [
    "def accuracy_by_bins(model, x, y):\n",
    "    \"\"\"\n",
    "    Calculate and print the accuracy of the given model for specific bins of classes.\n",
    "    The bins are defined as: 0-1, 2-7, 8-9.\n",
    "\n",
    "    :param model: Trained Keras model to evaluate.\n",
    "    :param x: Input features, shape of [num_samples, feature_dim].\n",
    "    :param y: One-hot encoded labels, shape of [num_samples, num_classes].\n",
    "    \"\"\"\n",
    "    predictions = model.predict(x).argmax(axis=-1)\n",
    "    true_labels = y.argmax(axis=-1)\n",
    "    bins = [(0, 1), (2, 7), (8, 9)]\n",
    "    for bin_start, bin_end in bins:\n",
    "        mask = (true_labels >= bin_start) & (true_labels <= bin_end) \n",
    "        bin_accuracy = np.mean(predictions[mask] == true_labels[mask])\n",
    "        print(f\"Accuracy for bin {bin_start}-{bin_end}: {bin_accuracy}\")\n",
    "\n",
    "print(\"Accuracy by bins for balanced data:\")\n",
    "print(\"Cross Entropy:\")\n",
    "accuracy_by_bins(model_ce, x_test, y_test)\n",
    "print(\"Focal Loss:\")\n",
    "accuracy_by_bins(model_fl, x_test, y_test)\n",
    "print(\"Rational Loss:\")\n",
    "accuracy_by_bins(model_rl, x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T19:51:35.653172800Z",
     "start_time": "2023-08-14T19:51:33.610674900Z"
    }
   },
   "id": "143062bb0fb2c0d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
