{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db106a0ce89886c6",
   "metadata": {},
   "source": [
    "# Practice on Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2390991af87b21",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# for types hints\n",
    "from typing import Tuple, Callable, List\n",
    "from tensorflow import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566659f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# check for gpus\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# list their names\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235572d8aa183bc7",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# mnist dataset\n",
    "def load_and_preprocess_mnist() -> Tuple[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor]]:\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset, preprocess images, and perform one-hot encoding of labels.\n",
    "\n",
    "    :return: Tuple of training data (x_train, y_train) and testing data (x_test, y_test).\n",
    "    \"\"\"\n",
    "    # Load MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) =  tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Reshape and normalize images\n",
    "    x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "    x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "    # One-hot encoding of labels\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff4c5d4d8c22ec",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_and_preprocess_mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49bafd834711d3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# building the model\n",
    "def build_model() -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Build a simple MLP model for MNIST classification.\n",
    "    :return: A tf.keras Model with inputs and outputs defined.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(784,), name='input')  # input layer\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='hidden1')(inputs)  # hidden layer\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='hidden2')(x)  # hidden layer\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax', name='output')(x)  # output layer\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_ce = build_model()\n",
    "model_fl = build_model()\n",
    "model_rl = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74658d8b5c82aac1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD()\n",
    "ce_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "# compiling the model\n",
    "model_ce.compile(optimizer=optimizer, loss=ce_loss, metrics=metrics)\n",
    "# training the model\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "model_ce.fit(x_train, y_train, batch_size=64, epochs=100, validation_split=0.2, callbacks=[early_stopping_callback])\n",
    "# evaluating the model\n",
    "test_scores = model_ce.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1]*100)\n",
    "\n",
    "# saving the model\n",
    "path = './weights/mnist_ce_model.keras'\n",
    "model_ce.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff849813ffddfd76",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "focal_loss = tf.keras.losses.CategoricalFocalCrossentropy()\n",
    "# compiling the model\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "model_fl.compile(optimizer=optimizer, loss=focal_loss, metrics=metrics)\n",
    "# training the model\n",
    "model_fl.fit(x_train, y_train, batch_size=64, epochs=100, validation_split=0.2, callbacks=[early_stopping_callback])\n",
    "# evaluating the model\n",
    "test_scores = model_fl.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1]*100)\n",
    "# saving the model\n",
    "path = './weights/mnist_focal_model.keras'\n",
    "model_fl.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb292d1ae3a65bc1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# defining rational loss function\n",
    "# RL(p_t) = 1/p_t * -p * log(p_t)\n",
    "    # TODO: plot the function\n",
    "def rational_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Rational Loss for multi-class classification, tf.keras style.\n",
    "    RL(p_t) = - 1/p_t * log(p_t), where p_t is the probability associated with the true class.\n",
    "\n",
    "    :param y_true: Ground truth labels, shape of [batch_size, num_classes].\n",
    "    :param y_pred: Predicted class probabilities, shape of [batch_size, num_classes].\n",
    "    :return: A scalar representing the mean rational loss over the batch.\n",
    "    NOTE: written assuming GPU support to make use of fast Tensor operations.\n",
    "    \"\"\"\n",
    "    # Create a Categorical Cross-Entropy loss instance\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(\n",
    "        reduction=tf.keras.losses.Reduction.NONE # Keep unreduced loss tensor\n",
    "    )\n",
    "    # clip the predicted probabilities to avoid log(0)\n",
    "    _y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0)\n",
    "    cross_entropy = cce(y_true, _y_pred) # batch_sizex1\n",
    "    # find the probability associated with the true class\n",
    "    _y_true = tf.argmax(y_true, axis=1)\n",
    "    # get the predicted probability of the true class\n",
    "    _y_pred = tf.gather(_y_pred, _y_true, axis=1)\n",
    "    # rational loss by dividing the cross entropy by the predicted probability of the true class\n",
    "    _rational_loss = cross_entropy / _y_pred # Rational loss\n",
    "    \n",
    "    return K.mean(_rational_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb936ed83a8b25",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# test rational loss\n",
    "y_true = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "# cross entropy value \n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "print(f'cross entropy: {cce(y_true, y_pred)}')\n",
    "print(f'cross entropy: {rational_loss(y_true, y_pred)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331edee2580725fd",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# def generate_data() -> Tuple[np.ndarray, np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     Generates synthetic y_true and y_pred data.\n",
    "#     :return: y_true and y_pred arrays.\n",
    "#     \"\"\"\n",
    "#     num_samples = 100\n",
    "#     num_classes = 3\n",
    "#     y_true = np.eye(num_classes)[np.random.choice(num_classes, num_samples)]\n",
    "#     y_pred = np.random.rand(num_samples, num_classes)\n",
    "#     y_pred /= y_pred.sum(axis=1, keepdims=True)\n",
    "#     return y_true, y_pred\n",
    "# \n",
    "# def plot_rational_loss() -> None:\n",
    "#     \"\"\"\n",
    "#     Plots the rational loss for the generated data.\n",
    "#     \"\"\"\n",
    "#     y_true, y_pred = generate_data()\n",
    "#     rational_loss_fixed = rational_loss()\n",
    "#     losses: List[float] = [rational_loss_fixed(y_t.reshape(1, -1), y_p.reshape(1, -1)).numpy() for y_t, y_p in zip(y_true, y_pred)]\n",
    "#     plt.plot(losses)\n",
    "#     plt.title(\"Rational Loss for Multi-Class Classification\")\n",
    "#     plt.xlabel(\"Sample\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.show()\n",
    "# \n",
    "# plot_rational_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab1b90eb796366",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model_rl.compile(optimizer= tf.keras.optimizers.SGD(), loss=rational_loss, metrics=metrics)\n",
    "# training the model\n",
    "history = model_rl.fit(x_train, y_train, batch_size=64, epochs=100, validation_split=0.2, callbacks=[early_stopping_callback])\n",
    "# evaluating the model\n",
    "test_scores = model_rl.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1]*100)\n",
    "# saving the model\n",
    "path = './weights/mnist_rational_model.keras'\n",
    "model_rl.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b6284c01bdf98",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# rebuilding the models\n",
    "model_ce = build_model()\n",
    "model_fl = build_model()\n",
    "model_rl = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c73a18c8f5973",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# imbalance \n",
    "def create_imbalanced_data(x, y, imbalance_rate=0.5):\n",
    "    \"\"\"\n",
    "    Create an imbalanced dataset based on a given probability distribution.\n",
    "    The probability for class d is given by: P(d) = 0.5^d / 2*(1 - 0.5^10)\n",
    "\n",
    "    :param x: Features, shape of [total_samples, feature_dim].\n",
    "    :param y: One-hot encoded labels, shape of [total_samples, num_classes].\n",
    "    :param imbalance_rate: Base rate for the exponential decay of class frequency (default 0.5).\n",
    "    :return: Tuple of imbalanced features and labels, shapes of [selected_samples, feature_dim] and [selected_samples, num_classes].\n",
    "    \"\"\"\n",
    "    total_samples = len(y)\n",
    "    a = imbalance_rate\n",
    "    normalization_factor = 2 * (1 - a**10)\n",
    "\n",
    "    indices_by_class = [np.where(y[:, d] == 1)[0] for d in range(10)]\n",
    "    selected_indices = []\n",
    "\n",
    "    for d in range(10):\n",
    "        probability_d = (a**d) / normalization_factor\n",
    "        frequency = int(total_samples * probability_d)\n",
    "        np.random.shuffle(indices_by_class[d]) # Shuffle to ensure random selection\n",
    "        selected_indices.extend(indices_by_class[d][:frequency])\n",
    "\n",
    "    return x[selected_indices], y[selected_indices]\n",
    "\n",
    "\n",
    "x_train_imbalanced, y_train_imbalanced = create_imbalanced_data(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d76b12905e4aa4",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model_ce.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
    "model_fl.compile(optimizer=tf.keras.optimizers.SGD(), loss=tf.keras.losses.CategoricalFocalCrossentropy(), metrics=metrics)\n",
    "model_rl.compile(optimizer=tf.keras.optimizers.SGD(), loss=rational_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefdf376d35ec93",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\"Training on imbalanced data:\")\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "print(\"Cross Entropy:\")\n",
    "model_ce.fit(x_train_imbalanced, y_train_imbalanced, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping_callback])\n",
    "print(\"Focal Loss:\")\n",
    "model_fl.fit(x_train_imbalanced, y_train_imbalanced, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping_callback])\n",
    "print(\"Rational Loss:\")\n",
    "model_rl.fit(x_train_imbalanced, y_train_imbalanced, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9401ce9ef65d9",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Balanced data\n",
    "print(\"test on balanced set:\")\n",
    "ce_res = model_ce.evaluate(x_test, y_test)\n",
    "fl_res = model_fl.evaluate(x_test, y_test)\n",
    "rl_res = model_rl.evaluate(x_test, y_test)\n",
    "print(f'ce: test loss {ce_res[0]}, test accuracy {ce_res[1]*100}')\n",
    "print(f'fl: test loss {fl_res[0]}, test accuracy {fl_res[1]*100}')\n",
    "print(f'rl: test loss {rl_res[0]}, test accuracy {rl_res[1]*100}')\n",
    "# Imbalanced data\n",
    "print(\"test on imbalanced data:\")\n",
    "x_test_imbalanced, y_test_imbalanced = create_imbalanced_data(x_test, y_test)\n",
    "ce_ires = model_ce.evaluate(x_test_imbalanced, y_test_imbalanced)\n",
    "fl_ires = model_fl.evaluate(x_test_imbalanced, y_test_imbalanced)\n",
    "rl_ires = model_rl.evaluate(x_test_imbalanced, y_test_imbalanced)\n",
    "print(f'ce: test loss {ce_ires[0]}, test accuracy {ce_ires[1]*100}')\n",
    "print(f'fl: test loss {fl_ires[0]}, test accuracy {fl_ires[1]*100}')\n",
    "print(f'rl: test loss {rl_ires[0]}, test accuracy {rl_ires[1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143062bb0fb2c0d0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def accuracy_by_bins(model, x, y):\n",
    "    \"\"\"\n",
    "    Calculate and print the accuracy of the given model for specific bins of classes.\n",
    "    The bins are defined as: 0-1, 2-7, 8-9.\n",
    "\n",
    "    :param model: Trained tf.keras model to evaluate.\n",
    "    :param x: Input features, shape of [num_samples, feature_dim].\n",
    "    :param y: One-hot encoded labels, shape of [num_samples, num_classes].\n",
    "    \"\"\"\n",
    "    predictions = model.predict(x).argmax(axis=-1)\n",
    "    true_labels = y.argmax(axis=-1)\n",
    "    bins = [(0, 1), (2, 7), (8, 9)]\n",
    "    for bin_start, bin_end in bins:\n",
    "        mask = (true_labels >= bin_start) & (true_labels <= bin_end) \n",
    "        bin_accuracy = np.mean(predictions[mask] == true_labels[mask])\n",
    "        print(f\"Accuracy for bin {bin_start}-{bin_end}: {bin_accuracy*100}\")\n",
    "\n",
    "print(\"Accuracy by bins for balanced data:\")\n",
    "print(\"Cross Entropy:\")\n",
    "accuracy_by_bins(model_ce, x_test, y_test)\n",
    "print(\"Focal Loss:\")\n",
    "accuracy_by_bins(model_fl, x_test, y_test)\n",
    "print(\"Rational Loss:\")\n",
    "accuracy_by_bins(model_rl, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
